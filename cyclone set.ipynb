{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cyclones\n",
    "\n",
    "## Objectives:\n",
    "1. Clean the data.\n",
    "2. Statistical Analysis of data.\n",
    " - *Find Top ten cyclone by frequency.*\n",
    " - *Find frequency of cyclones by month.*\n",
    " - *Find frequency of cyclones by year.*\n",
    " - *Find frequency of cyclones by category.*\n",
    "3. Classification into cyclones using Logistic Regression, Decision Tree, Random Forrest , Naive Bayes and SVM.\n",
    " - *Perform Feature selection using Random Forest.* \n",
    " - *Compare the prediction by Decision Tree Model performance using all the features and top five features.* \n",
    " - *Find the prediction accuracy of Random Forest Model model using the top five features.* \n",
    " - *Compare the prediction by Naive Bayes Model performance using all the features and top five features.* \n",
    " - *Find the prediction accuracy of SVM model using the top five features.* \n",
    " - *Show which model has performed the best.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore Warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status can be the following types:\n",
    "# TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n",
    "# TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n",
    "# HU – Tropical cyclone of hurricane intensity (> 64 knots)\n",
    "# EX – Extratropical cyclone (of any intensity)\n",
    "# SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n",
    "# SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n",
    "# LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n",
    "# WV – Tropical Wave (of any intensity)\n",
    "# DB – Disturbance (of any intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the libraries.\n",
    "\n",
    "# Import pandas.\n",
    "import pandas as pd\n",
    "# Import numpy.\n",
    "import numpy as np\n",
    "# Import matplotlib.\n",
    "import matplotlib.pyplot as plt\n",
    "# Import seaborn.\n",
    "import seaborn as sns\n",
    "# Import regular expression.\n",
    "import re\n",
    "# import datetime.\n",
    "import datetime as dt\n",
    "# Import the data.\n",
    "df = pd.read_csv('/kaggle/input/hurricane-database/pacific.csv')\n",
    "# Convert date column as datetime.\n",
    "df['Date'] = pd.to_datetime(df['Date'] , format= '%Y%m%d')\n",
    "\n",
    "# I want to create columns Latitude Hemisphere and Longitude Hemisphere with code 0 = N , 1 = S & 0 = E , 1 = W.\n",
    "def hemisphere(coord):\n",
    "        hem = re.findall(r'[NSWE]' , coord)[0]\n",
    "        if hem == 'N' or hem == 'E':\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "# Creating the column Latitude_Hemisphere.    \n",
    "df['Latitude_Hemisphere'] = df['Latitude'].apply(hemisphere)\n",
    "df['Longitude_Hemisphere'] = df['Longitude'].apply(hemisphere)\n",
    "df['Latitude_Hemisphere'] = df['Latitude_Hemisphere'].astype('category')\n",
    "df['Longitude_Hemisphere'] = df['Longitude_Hemisphere'].astype('category')\n",
    "\n",
    "# Convert the latitude and longitude Column to numeric type.\n",
    "df['Latitude'] =  df['Latitude'].apply(lambda x: re.match('[0-9]{1,3}.[0-9]{0,1}' , x)[0])\n",
    "df['Longitude'] =   df['Longitude'].apply(lambda x: re.match('[0-9]{1,3}.[0-9]{0,1}' , x)[0])\n",
    "\n",
    "# The missing values are given by -999. So , we need to fill them appropriately.\n",
    "\n",
    "# Show the count of missing values and fill them with mean.\n",
    "for column in df.columns:\n",
    "    missing_cnt = df[column][df[column] == -999].count()\n",
    "    print('Missing Values in column {col} = '.format(col = column) , missing_cnt )\n",
    "    if missing_cnt!= 0:\n",
    "#         print('in ' , column)\n",
    "        mean = round(df[column][df[column] != -999 ].mean())\n",
    "#         print(\"mean\",mean)\n",
    "        index = df.loc[df[column] == -999 , column].index\n",
    "#         print(\"index\" , index )\n",
    "        df.loc[df[column] == -999 , column] = mean\n",
    "#         print(df.loc[index , column])\n",
    "        \n",
    "# Restructure the dataframe for visibility and remove columns ID and Event.        \n",
    "df =  df[['ID', 'Name', 'Date', 'Time', 'Event', 'Status', 'Latitude', 'Latitude_Hemisphere' , \n",
    "       'Longitude', 'Longitude_Hemisphere' ,'Maximum Wind', 'Minimum Pressure', 'Low Wind NE',\n",
    "       'Low Wind SE', 'Low Wind SW', 'Low Wind NW', 'Moderate Wind NE',\n",
    "       'Moderate Wind SE', 'Moderate Wind SW', 'Moderate Wind NW',\n",
    "       'High Wind NE', 'High Wind SE', 'High Wind SW', 'High Wind NW']]\n",
    "\n",
    "# Change all time to format HHMM.\n",
    "df['Time'] = df['Time'].astype('object')\n",
    "def hhmm(time):\n",
    "    time = str(time)\n",
    "    digits = re.findall(r'\\d', time)\n",
    "    t = ''\n",
    "    if len(digits) == 1:\n",
    "        t ='0{i}00'.format(i =time)\n",
    "    elif len(digits) == 2:\n",
    "        t = '{i}00'.format(i =time)\n",
    "    elif len(digits) == 3:\n",
    "        t = '0{i}'.format(i =time)\n",
    "    else:\n",
    "        t = time\n",
    "    return t\n",
    "# Apply the function.\n",
    "df['Time'] = df['Time'].apply(hhmm)\n",
    "\n",
    "# Convert the column into Datetime.\n",
    "df['Time'] = pd.to_datetime(df['Time'] , format='%H%M').dt.time\n",
    "\n",
    "\n",
    "# Convert the status column to categorical.\n",
    "df['Status'] = df['Status'].astype('category')\n",
    "\n",
    "data = df.drop(columns = ['ID' , 'Event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statististical Analysis of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top ten cyclones which occured the maximum number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top ten cyclones which have occured the maximum number of times.\n",
    "lst = [x.strip() for x in data.groupby('Name').count().sort_values(by = 'Date' , ascending = False).index[:10]]\n",
    "val = data.groupby('Name').count().sort_values(by = 'Date' , ascending = False)[:10]['Date'].values\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "fig , ax = plt.subplots()\n",
    "fig.set_size_inches(12,12)\n",
    "ax.pie(  labels = lst , x = val , autopct='%.1f%%' , explode = [0.1 for x in range(10)])\n",
    "plt.title(' Top Ten Cyclones  by Frequency.' , fontsize = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of Hurricanes by Month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Month'] = data['Date'].apply(lambda x: x.month)\n",
    "data['Year'] = data['Date'].apply(lambda x: x.year)\n",
    "mnt = ['Jan' , 'Feb' , 'Mar' , 'Apr' , 'May' , 'June' , 'July' , 'Aug' , 'Sep','Oct' , 'Nov' , 'Dec']\n",
    "temp = data.groupby('Month').count()\n",
    "temp.loc[4] = 0\n",
    "temp = temp.sort_values(by = 'Month' , ascending = False)\n",
    "font = {'family' : 'monospace',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x = temp.index , y = 'Date' , data=temp , palette = 'RdBu' )\n",
    "plt.xticks([0,1,2,3,4,5,6,7,8,9,10,11] , mnt , rotation = 90)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Cyclones by Month.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year Wise Frequency of Hurricanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year-Wise Frequency of Hurricanes.\n",
    "temp = data.groupby('Year').count().sort_values(by = 'Month' , ascending = False)\n",
    "plt.figure(figsize= (12,12))\n",
    "sns.lineplot(x = temp.index , y = 'Month' , data = temp , label = 'Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Year Wise Frequency of Hurricanes.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution Function of Frequency.\n",
    "temp = data.groupby('Year').count().sort_values(by = 'Date' , ascending = False)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.distplot(temp['Date'].values , norm_hist = True , axlabel = 'Probability Distribution of Frequency of Cyclones.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of Cyclones by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency of Cyclones by Category\n",
    "# TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n",
    "# TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n",
    "# HU – Tropical cyclone of hurricane intensity (> 64 knots)\n",
    "# EX – Extratropical cyclone (of any intensity)\n",
    "# SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n",
    "# SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n",
    "# LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n",
    "# WV – Tropical Wave (of any intensity)\n",
    "# DB – Disturbance (of any intensity)\n",
    "temp = data.groupby('Status').count().sort_values(by = 'Date' , ascending = False)\n",
    "fig , ax = plt.subplots()\n",
    "fig.set_size_inches(12,12)\n",
    "sns.barplot(y = list(temp.index) , x = 'Date' , data = temp, palette= 'pastel' )\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Catehory')\n",
    "plt.title('Category wise Frequency Distribution of Cyclones.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Classification model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Decision Tree Classifier.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import train-test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import accuracy Score.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Import Recall Score.\n",
    "from sklearn.metrics import recall_score \n",
    "\n",
    "#Import Precision Score.\n",
    "from sklearn.metrics import precision_score \n",
    "\n",
    "# Form the model.\n",
    "dt = DecisionTreeClassifier(min_samples_leaf=50 , criterion='entropy')\n",
    "\n",
    "\n",
    "# Set the dependent and independent variables.\n",
    "x_train = data[['Latitude', 'Latitude_Hemisphere',\n",
    "       'Longitude', 'Longitude_Hemisphere', 'Maximum Wind', 'Minimum Pressure',\n",
    "       'Low Wind NE', 'Low Wind SE', 'Low Wind SW', 'Low Wind NW',\n",
    "       'Moderate Wind NE', 'Moderate Wind SE', 'Moderate Wind SW',\n",
    "       'Moderate Wind NW', 'High Wind NE', 'High Wind SE', 'High Wind SW',\n",
    "       'High Wind NW' , 'Month' , 'Year']]\n",
    "y_train = data['Status']\n",
    "\n",
    "\n",
    "# Perform the Kfold validation.\n",
    "\n",
    "# Import the KFold library.\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10 , shuffle= True , random_state=42 )\n",
    "\n",
    "dt_scores = []\n",
    "dt_recall_scores = []\n",
    "dt_precision_scores = []\n",
    "for tr , ts in kf.split(x_train):\n",
    "    xtr = x_train.loc[tr]\n",
    "    ytr = y_train.loc[tr]\n",
    "    xts = x_train.loc[ts]\n",
    "    yts = y_train.loc[ts]\n",
    "    dt.fit(xtr , ytr)\n",
    "    y_pred = dt.predict(xts) \n",
    "    dt_scores.append(accuracy_score(yts, y_pred)) \n",
    "    dt_recall_scores.append(recall_score(yts , y_pred , average = 'weighted'))\n",
    "    dt_precision_scores.append(precision_score(yts , y_pred , average = 'weighted'))\n",
    "# dt.fit(x_train, y_train)\n",
    "# y_pred = dt.predict(x_test)\n",
    "# accuracy_score(y_test, y_pred)\n",
    "dt_scr = {'accuracy' : np.mean(dt_scores) , 'recall': np.mean(dt_recall_scores) , 'precision' :  np.mean(dt_precision_scores) }\n",
    "print('Accuracy score for Decision Tree is :' , dt_scr['accuracy'])\n",
    "print('Recall score for Decision Tree is :' , dt_scr['recall'])\n",
    "print('Precision score for Decision Tree is :' , dt_scr['precision'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# First I want to determine the important features.\n",
    "rf = RandomForestClassifier(oob_score=True , n_estimators=1000)\n",
    "rf.fit(x_train , y_train)\n",
    "features = pd.Series(rf.feature_importances_ , index= x_train.columns).sort_values(ascending=False)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a decision tree for top ten most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top ten most important features.\n",
    "features.index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dependent and independent variables.\n",
    "x_trainf = data[features.index[:5]]\n",
    "y_train = data['Status']\n",
    "\n",
    "# Perform the Kfold validation.\n",
    "\n",
    "# Import the KFold library.\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10 , shuffle= True , random_state=42 )\n",
    "\n",
    "dt_scores = []\n",
    "dt_recall_scores = []\n",
    "dt_precision_scores = []\n",
    "for tr , ts in kf.split(x_trainf):\n",
    "    xtr = x_trainf.loc[tr]\n",
    "    ytr = y_train.loc[tr]\n",
    "    xts = x_trainf.loc[ts]\n",
    "    yts = y_train.loc[ts]\n",
    "    dt.fit(xtr , ytr)\n",
    "    y_pred = dt.predict(xts) \n",
    "    dt_scores.append(accuracy_score(yts, y_pred)) \n",
    "    dt_recall_scores.append(recall_score(yts , y_pred , average = 'weighted'))\n",
    "    dt_precision_scores.append(precision_score(yts , y_pred , average = 'weighted'))\n",
    "# dt.fit(x_train, y_train)\n",
    "# y_pred = dt.predict(x_test)\n",
    "# accuracy_score(y_test, y_pred)\n",
    "dt_scr5 = {'accuracy' : np.mean(dt_scores) , 'recall': np.mean(dt_recall_scores) , 'precision' :  np.mean(dt_precision_scores) }\n",
    "print('Accuracy score for Decision Tree is :' , dt_scr['accuracy'])\n",
    "print('Recall score for Decision Tree is :' , dt_scr['recall'])\n",
    "print('Precision score for Decision Tree is :' , dt_scr['precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see the Top five features('Maximum Wind', 'Minimum Pressure', 'Latitude', 'Year', 'Longitude') give the same accuracy as when we get choosing all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 . Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here instead of cross validation we will be using oob score as a measure of accuracy.\n",
    "# I will hyper tuning the parameter: No of Trees.\n",
    "\n",
    "trees  = [10, 20 , 50, 100,200,500,1000,1200]\n",
    "maxn_five = {}\n",
    "maxn = {}\n",
    "for i in trees:\n",
    "    rf = RandomForestClassifier(n_estimators=i , oob_score=True)\n",
    "    rf.fit(x_trainf , y_train)\n",
    "    print('Obb Score for {x} trees: and taking top five features '.format(x = i) , rf.oob_score_)\n",
    "    maxn_five[i] = rf.oob_score_\n",
    "    rf.fit(x_trainf , y_train)\n",
    "    print('Obb Score for {x} trees: and taking all the features '.format(x = i) , rf.oob_score_)\n",
    "    maxn[i] = rf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing.\n",
    "x_trains , x_tests , y_trains, y_tests  = train_test_split(x_trainf, y_train, test_size=0.33, random_state=42)\n",
    "# Set n to the feature of maximum oob score.\n",
    "n = 0\n",
    "for i in maxn_five:\n",
    "    if max(maxn_five.values()) == maxn_five[i]:\n",
    "        n= i\n",
    "# Set n_estimators to n.\n",
    "rf = RandomForestClassifier(oob_score=True , n_estimators=n)\n",
    "rf.fit(x_trains , y_trains)\n",
    "y_pred_rf = rf.predict(x_tests[features.index[:5]])\n",
    "scores_rf = {'accuracy': accuracy_score(y_tests , y_pred_rf) ,'recall' : recall_score(y_tests , y_pred_rf , average='weighted') ,'precision' : precision_score(y_tests , y_pred_rf , average='weighted') }\n",
    "print('Scores for Random Forest with n = ' , n , ' and using features ',  features.index[:5] , ' are : ')\n",
    "print('Accuracy: ' , scores_rf['accuracy'])\n",
    "print('Recall: ' , scores_rf['recall'])\n",
    "print('Precision: ' , scores_rf['precision'])\n",
    "\n",
    "# n_All = 0\n",
    "# for i in maxn:\n",
    "#     if max(maxn.values()) == maxn[i]:\n",
    "#         n_All= i\n",
    "# # Set n_estimators to n.\n",
    "# rf = RandomForestClassifier(oob_score=True , n_estimators=n_All)\n",
    "# rf.fit(x_train , y_train)\n",
    "# y_pred_rf_all = rf.predict(x_test)\n",
    "# scores_rf_all = {'accuracy': accuracy_score(y_test , y_pred_rf) ,'recall' : recall_score(y_test , y_pred_rf , average='weighted') ,'precision' : precision_score(y_test , y_pred_rf , average='weighted') }\n",
    "# print('Scores for Random Forest with n = ' , n_All , ' and using all features ' , ' are : ')\n",
    "# print('Accuracy: ' , scores_rf_all['accuracy'])\n",
    "# print('Recall: ' , scores_rf_all['recall'])\n",
    "# print('Precision: ' , scores_rf_all['precision'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naive Bayes Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb5 = GaussianNB()\n",
    "acc_s = [] \n",
    "rcl_s = [] \n",
    "ps_scr = []\n",
    "acc_s_5 = [] \n",
    "rcl_s_5 = [] \n",
    "ps_scr_5 = []\n",
    "for tr, ts in kf.split(x_train):\n",
    "    xtr = x_train.loc[tr]\n",
    "    ytr = y_train.loc[tr]\n",
    "    xts = x_train.loc[ts]\n",
    "    yts = y_train.loc[ts]\n",
    "    xtr5 = x_trainf.loc[tr]\n",
    "    xts5 = x_trainf.loc[ts]\n",
    "\n",
    "    \n",
    "    # Accuracy , Precision and recall with all features.\n",
    "   \n",
    "    nb.fit(xtr , ytr)\n",
    "    y_nb_pred = nb.predict(xts)\n",
    "    acc_s.append(accuracy_score(yts , y_nb_pred))\n",
    "    rcl_s.append(recall_score(yts , y_nb_pred , average = 'weighted'))\n",
    "    ps_scr.append(precision_score(yts , y_nb_pred , average = 'weighted'))\n",
    "    \n",
    "#     Accuracy , Precision and recall with top five features.\n",
    "    nb5.fit(xtr5 , ytr)\n",
    "    y_nb5_pred = nb5.predict(xts5)\n",
    "    acc_s_5.append(accuracy_score(yts , y_nb5_pred))\n",
    "    rcl_s_5.append(recall_score(yts , y_nb5_pred , average = 'weighted'))\n",
    "    ps_scr_5.append(precision_score(yts , y_nb5_pred , average = 'weighted'))\n",
    "    \n",
    "nb_scores = {'accuracy':np.mean(acc_s) , 'recall':np.mean(rcl_s) , 'precision':np.mean(ps_scr)}\n",
    "nb5_scores = {'accuracy':np.mean(acc_s_5) , 'recall':np.mean(rcl_s_5) , 'precision':np.mean(ps_scr_5)}\n",
    "print('Naive Bayes results for top five features for Accuracy ' , nb5_scores['accuracy'] , 'Recall: ' , nb5_scores['recall'], 'and Precision: ' , nb5_scores['precision'] )\n",
    "print('Naive Bayes results for all features for Accuracy ' , nb_scores['accuracy'] , 'Recall: ' , nb_scores['recall'], 'and Precision: ' , nb_scores['precision'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the overall score with top five features is significantly greater than the overall score with all the features. Hence , we can see that feature selection is very important for Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Support Vector Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SVM.\n",
    "from sklearn import svm\n",
    "mdl5 = svm.SVC()\n",
    "acc_s_5 = [] \n",
    "rcl_s_5 = [] \n",
    "ps_scr_5 = []\n",
    "\n",
    "# Split the data into train and test.\n",
    "xtr5, xts5 , ytr , yts = train_test_split(x_trainf , y_train , test_size = 0.25 , random_state = 42)\n",
    "\n",
    "# Train the model.\n",
    "mdl5.fit(xtr5 , ytr)\n",
    "y_mdl5_pred = nb5.predict(xts5)\n",
    "acc_s_5.append(accuracy_score(yts , y_mdl5_pred))\n",
    "rcl_s_5.append(recall_score(yts , y_mdl5_pred , average = 'weighted'))\n",
    "ps_scr_5.append(precision_score(yts , y_mdl5_pred , average = 'weighted'))\n",
    "\n",
    "# for tr, ts in kf.split(x_train):\n",
    "#     ytr = y_train.loc[tr]\n",
    "#     yts = y_train.loc[ts]\n",
    "#     xtr5 = x_trainf.loc[tr]\n",
    "#     xts5 = x_trainf.loc[ts]\n",
    "\n",
    "# #   Accuracy , Precision and recall with top five features.\n",
    "#     mdl5.fit(xtr5 , ytr)\n",
    "#     y_mdl5_pred = nb5.predict(xts5)\n",
    "#     acc_s_5.append(accuracy_score(yts , y_mdl5_pred))\n",
    "#     rcl_s_5.append(recall_score(yts , y_mdl5_pred , average = 'weighted'))\n",
    "#     ps_scr_5.append(precision_score(yts , y_mdl5_pred , average = 'weighted'))\n",
    "    \n",
    "svm_scores = {'accuracy':np.mean(acc_s_5) , 'recall':np.mean(rcl_s_5) , 'precision':np.mean(ps_scr_5)}\n",
    "print('SVM results for top five features for Accuracy ' , svm_scores['accuracy'] , 'Recall: ' , svm_scores['recall'], 'and Precision: ' , svm_scores['precision'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the overall score with top five features is significantly greater than the overall score with all the features. Hence , we can see that feature selection is very important for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing the algorithms.\n",
    "res = {'DecisionTree':dt_scr5['accuracy'] , 'RandomForest': scores_rf['accuracy'] , 'GaussianNB': nb5_scores['accuracy'] , 'SVM':svm_scores['accuracy']}\n",
    "max_res = max(res.values())\n",
    "max_index = ''\n",
    "for i in res:\n",
    "    if res[i] == max_res:\n",
    "        max_index = i\n",
    "print('The most effictive algorithm is :' , max_index , 'with accuracy: ' , res[max_index])        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
